# ğŸ–¼ï¸ Stable Diffusion â€” Custom Implementation

This repository contains a minimal, modular, and fully Python-based implementation of **Stable Diffusion v1.5**, including CLIP text encoding, the UNet + DDPM denoising process, and VAE encoding/decoding.  
It uses the official **v1-5-pruned-emaonly.ckpt** model weights and supports prompt-based image generation.

---

## ğŸš€ Features

- âœ”ï¸ End-to-end Stable Diffusion v1.5 inference  
- âœ”ï¸ CLIP tokenizer + text encoder (using `vocab.json` and `merges.txt`)  
- âœ”ï¸ VAE encoder/decoder fully implemented from scratch  
- âœ”ï¸ UNet with 2D attention and cross-attention  
- âœ”ï¸ DDPM scheduler with classifier-free guidance  
- âœ”ï¸ Modular and easy-to-read architecture  

---

## ğŸ”— Google Colab Notebook

You can try the full working implementation directly in Google Colab:

ğŸ‘‰ **[Open the Colab Notebook](https://colab.research.google.com/github/VasuVerma990/Stable-Diffusion/blob/_Main_.ipynb)**


---

## ğŸ”„ Pipeline Overview

The Stable Diffusion inference pipeline follows the official v1.5 process:

1. **Tokenize prompt** (BPE using `vocab.json` + `merges.txt`)  
2. **Encode text** using the CLIP text transformer  
3. **Sample initial Gaussian noise** in latent space  
4. **Iteratively denoise** using UNet + DDPM scheduler  
5. **Decode final latents** into an RGB image with the VAE decoder  

---

## ğŸ“ Notes

- This repository emphasizes **clarity and modularity** in implementation.  
- Includes full support for **classifier-free guidance**.  
- Requires **`vocab.json`** and **`merges.txt`** for CLIP tokenizer functionality.  

---

